{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 (Total Points: 175)\n",
    "\n",
    "\n",
    "\n",
    "Learning Goals:\n",
    "- Learn how to load a dataset and process it.\n",
    "- Learn how to implement several IR methods (TFIDF, BM25, QL) and understand their weaknesses & strengths.\n",
    "- Learn how to evaluate IR methods\n",
    "\n",
    "\n",
    "**NOTE 1**: Only the code (`TODO: Implement this!` denotes these sections) is graded. The 'theory' questions in this assignment serve as a preparation for the exam and to facilitate a deeper understanding of the course content. These questions (denoted by `TODO: Answer this!`) have no points assigned to them, but **need** to be filled out before submission.  \n",
    "\n",
    "**NOTE 2**: You can use the `nltk`, `numpy` and `matplotlib` libraries here. Other libraries, e.g., `gensim` or `scikit-learn`, may not be used. \n",
    "\n",
    "**NOTE 3**: The notebook you submit has to have the student ids, seperated by underscores (E.g., `12341234_12341234_12341234.ipynb`). \n",
    "\n",
    "**NOTE 4**: Make sure to check that your notebook runs before submission. A quick way to do this is to restart the kernel and run all the cells.  \n",
    "\n",
    "---\n",
    "Additional Resources: \n",
    "-  Sections 2.3, 4.1, 4.2, 4.3, 5.3, 5.6, 5.7, 6.2, 7, 8 of [Search Engines: Information Retrieval in Practice](https://ciir.cs.umass.edu/downloads/SEIRiP.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  References (used to answer the theoretical questions)\n",
    "- [Jaime Arguello] https://ils.unc.edu/courses/2013_spring/inls509_001/lectures/10-EvaluationMetrics.pdf\n",
    "\n",
    "- [Croft *et al.*] https://ciir.cs.umass.edu/downloads/SEIRiP.pdf\n",
    "\n",
    "- [Manning *et al.*] https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "# TODO: Ensure that no additional library is imported in the notebook. \n",
    "# TODO: Only the standard library and the following libraries are allowed:\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "from functools import partial\n",
    "\n",
    "import nltk\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display, HTML\n",
    "from IPython.html import widgets\n",
    "from collections import namedtuple\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Text Processing (20 points)\n",
    "\n",
    "In this section, we will load the dataset and learn how to clean up the data to make it usable for an IR system. \n",
    "\n",
    "We are using the [CACM dataset](http://ir.dcs.gla.ac.uk/resources/test_collections/cacm/), which is a small, classic IR dataset, composed of a collection of titles and abstracts from the journal CACM. It comes with relevance judgements for queries, so we can evaluate our IR system. \n",
    "\n",
    "The following cell downloads the dataset and unzips it to a local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(folder_path = \"./datasets/\"):\n",
    "    \n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    file_location = os.path.join(folder_path, \"cacm.zip\")\n",
    "    \n",
    "    # download file if it doesn't exist\n",
    "    if not os.path.exists(file_location):\n",
    "        \n",
    "        url = \"https://surfdrive.surf.nl/files/index.php/s/M0FGJpX2p8wDwxR/download\"\n",
    "\n",
    "        with open(file_location, \"wb\") as handle:\n",
    "            print(f\"Downloading file from {url} to {file_location}\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            for data in tqdm(response.iter_content()):\n",
    "                handle.write(data)\n",
    "            print(\"Finished downloading file\")\n",
    "    \n",
    "    if not os.path.exists(os.path.join(folder_path, \"train.txt\")):\n",
    "        \n",
    "        # unzip file\n",
    "        with zipfile.ZipFile(file_location, 'r') as zip_ref:\n",
    "            zip_ref.extractall(folder_path)\n",
    "        \n",
    "download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a brief description of each file in the dataset by looking at the README file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Read the README file \n",
    "# !cat ./datasets/README\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "We are interested in 4 files:\n",
    "- `cacm.all` : Contains the text for all documents. Note that some documents do not have abstracts available. \n",
    "- `query.text` : The text of all queries\n",
    "- `qrels.text` : The relevance judgements\n",
    "- `common_words` : A list of common words. This may be used as a collection of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### The first 45 lines of the CACM dataset forms the first record\n",
    "# We are interested only in 3 fields. \n",
    "# 1. the '.I' field, which is the document id\n",
    "# 2. the '.T' field (the title) and\n",
    "# 3. the '.W' field (the abstract, which may be absent)\n",
    "# !head -45 ./datasets/cacm.all\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now, write a function to read in the `cacm.all` file. Note that each document has a variable number of lines. The `.I` field denotes a new document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! (4 points)\n",
    "def read_cacm_docs(root_folder = \"./datasets/\"):\n",
    "    \"\"\"\n",
    "        Reads in the CACM documents. The dataset is assumed to be in the folder \"./datasets/cacm\" be default\n",
    "        Returns: A list of 2-tuples: (doc_id, document), where 'document' is a single string created by \n",
    "            appending the title and abstract (seperated by a \"\\n\"). \n",
    "            In case the record doesn't have an abstract, the document is composed only by the title\n",
    "    \"\"\"\n",
    "     \n",
    "    with open(os.path.join(root_folder,\"cacm.all\")) as file:\n",
    "        \n",
    "        text = file.read()\n",
    "        documents = text.split(\".I \")[1:]\n",
    "        \n",
    "        res = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            \n",
    "            split_doc = doc.split(\"\\n.\")\n",
    "            \n",
    "            ID = split_doc[0]\n",
    "            \n",
    "            T = [d for d in split_doc if d[0]==\"T\"][0][2:]\n",
    "            \n",
    "            W = [d for d in split_doc if d[0]==\"W\"]\n",
    "            if W: T+= \"\\n\" + W[0][2:]\n",
    "            \n",
    "            res.append((ID, T))\n",
    "\n",
    "    return res\n",
    "    \n",
    "docs = read_cacm_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### \n",
    "assert len(docs) == 3204, \"There should be exactly 3024 documents\"\n",
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next, let us read the queries. They are formatted similarly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".I 1\r\n",
      ".W\r\n",
      " What articles exist which deal with TSS (Time Sharing System), an\r\n",
      "operating system for IBM computers?\r\n",
      ".N\r\n",
      " 1. Richard Alexander, Comp Serv, Langmuir Lab (TSS)\r\n",
      " \r\n",
      ".I 2\r\n",
      ".W\r\n",
      " I am interested in articles written either by Prieve or Udo Pooch\r\n",
      ".A\r\n",
      "Prieve, B.\r\n",
      "Pooch, U.\r\n",
      ".N\r\n",
      " 2. Richard Alexander, Comp Serv, Langmuir Lab (author = Pooch or Prieve)\r\n"
     ]
    }
   ],
   "source": [
    "##### The first 15 lines of 'query.text' has 2 queries\n",
    "# We are interested only in 2 fields. \n",
    "# 1. the '.I' - the query id\n",
    "# 2. the '.W' - the query\n",
    "# 3. the '.W' field (the abstract, which may be absent)\n",
    "!head -15 ./datasets/query.text\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now, write a function to read in this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! (3 points)\n",
    "def read_queries(root_folder = \"./datasets/\"):\n",
    "    \"\"\"\n",
    "        Reads in the CACM queries. The dataset is assumed to be in the folder \"./datasets/\" be default\n",
    "        Returns: A list of 2-tuples: (query_id, query)\n",
    "    \"\"\"\n",
    "    \n",
    "    queries = [] \n",
    "    with open(os.path.join(root_folder,\"query.text\")) as file:\n",
    "        \n",
    "        queries_string = file.read()\n",
    "        \n",
    "        for q in queries_string.split(\".I \")[1:]:\n",
    "            \n",
    "            \"\"\"\n",
    "            #### OLD:\n",
    "            q_ = q.split('.W\\n ')\n",
    "            \n",
    "            query_id = q_[0].strip('\\n')\n",
    "            \n",
    "            query = q_[1].split(\"\\n.N\")[0].replace('\\n', ' ').replace('/', '')\n",
    "    \n",
    "            queries.append((query_id, query))\n",
    "            \"\"\"\n",
    "            \n",
    "            split_query = q.split(\"\\n.\")\n",
    "            \n",
    "            ID = split_query[0]\n",
    "            \n",
    "            W = [q for q in split_query if q[0]==\"W\"][0][2:]\n",
    "            W = W.replace('\\n', ' ').replace('/', '')\n",
    "            \n",
    "            queries.append((ID, W))\n",
    "            \n",
    "            \n",
    "    return queries\n",
    "   \n",
    "queries = read_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### \n",
    "assert len(queries) == 64 and all([q[1] is not None for q in queries]), \"There should be exactly 64 queries\"\n",
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Read in the stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\r\n",
      "about\r\n",
      "above\r\n",
      "accordingly\r\n",
      "across\r\n",
      "after\r\n",
      "afterwards\r\n",
      "again\r\n",
      "against\r\n",
      "all\r\n"
     ]
    }
   ],
   "source": [
    "!head ./datasets/common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! (3 points)\n",
    "def load_stopwords(root_folder = \"./datasets/\"):\n",
    "    \"\"\"\n",
    "    Load the stopwords\n",
    "    Output: A set of stopwords\n",
    "    \"\"\"\n",
    "    with open(root_folder+'common_words') as file:\n",
    "        text = file.read()\n",
    "        words = text.split('\\n')\n",
    "        words.remove('')\n",
    "        return list(set(words))\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "assert len(stopwords) == 428"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "We can now write some basic text processing functions. A first step is to tokenize the text. You may use any tokenizer available in the `nltk` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/david/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement this! (5 points)\n",
    "nltk.download('punkt')\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "        Tokenize the text. \n",
    "        Input: text - a string\n",
    "        Output: a list of tokens\n",
    "    \"\"\"\n",
    "    return nltk.tokenize.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "text = \"the quick brown fox jumps over the lazy dog.\"\n",
    "tokens = tokenize(text)\n",
    "print(tokens)\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*Answer the following questions*: \n",
    "- Why is stemming necessary, in particular for IR?\n",
    "    - Stemming is the process of reducing inflected and derived forms of a word to a common basic form. This is done by using a set of fixed rules (for example always removing the ending -ings) and applying it to all words of all documents. This way, we can identify documents with similar content that only used different word forms. To exemplify, we can find that documents that contain the words \"democratic\", \"democracy\" or \"democratization\" are all relevant for a query that contains the word \"democrat\". This can help to increase the recall of our search.\n",
    "      \n",
    "- Is there any setting (domain, scenario, etc) in which stemming can hurt performance? Illustrate with an example\n",
    "    - When we perform stemming we perform a generalisation of words, which implicitly means that we might decrease our precision. Consider for example the case that we have a query containing the word \"withing\". When our stemming algorithm removes the ending \"-ing\" whenever present, we will now find all documents that contain the word \"with\" to be relevant. Similarily, think of the words \"experiment\" and \"experience\", who might both be stemmend to \"experi\" and therefore wrongly associated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to stem tokens. Again, you can use the `nltk` library for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! (5 points)\n",
    "def stem_token(token):\n",
    "    \"\"\"\n",
    "        Stem the given token, using any stemmer available from the nltk library\n",
    "        Input: a single token\n",
    "        Output: the stem of the token\n",
    "    \"\"\"\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    return stemmer.stem(token)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog', '.']\n",
      "['caress', 'fli', 'die', 'mule', 'deni', 'die', 'agre', 'own', 'humbl', 'size', 'meet', 'state', 'siez', 'item', 'sensat', 'tradit', 'refer', 'colon', 'plot']\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "print([stem_token(t) for t in tokens])\n",
    "tokens_ = [\n",
    "    'caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "    'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "    'meeting', 'stating', 'siezing', 'itemization',\n",
    "    'sensational', 'traditional', 'reference', 'colonizer',\n",
    "    'plotted']\n",
    "print([stem_token(t) for t in tokens_])\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*Answer the following questions*: \n",
    "- Another processing step (not done here) is to use n-grams. Illustrate why you would want to use n-grams in IR with an example.  \n",
    "    - Currently, we represent every documement as a bag-of-words which means that we completely disregard the word order. This is different when using a n-gram language model, where we represent the document as a bag of sequences of length n. By doing so, we implicitly model more context and will most likely end up with a more expressive representation of the document's content. Consequently, we can find documents that contain phrases from the query. Croft et al. give the example of the query \"black sea\". Clearly, documents that contain \"black sea\" are more likely to be relevant to our query than documents containing the words \"black\"and \"sea\" individually.\n",
    "   \n",
    "- Usage of n-grams exacerbates some problems ex. in bi-gram language models. What is this problem? Suggest one solution \n",
    "    - The impact of phrases on the relevance of documents might not always be as clear as in the \"black sea\" example. For example when we consider the query \"art supplies\", a  document that does not contain the exact query but the words \"art\"/\"arts\"/\"artist\"/\"supplies\"/\"supply\"/etc. individually might be very relevant. A possible solution would be to consider both bigrams and single stemmed words contained in documents and then weight the occurences against each other to obtain an overall relevance ranking.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "The following function puts it all together. Given a string, it tokenizes it, and processes it according to the flags that you set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Putting it all together\n",
    "def process_text(text, stem=False, remove_stopwords=False, lowercase_text=False):\n",
    "    \n",
    "    tokens = []\n",
    "    for token in tokenize(text):\n",
    "        if remove_stopwords and token.lower() in stopwords:\n",
    "            continue\n",
    "        if stem:\n",
    "            token = stem_token(token)\n",
    "        if lowercase_text:\n",
    "            token = token.lower()\n",
    "        tokens.append(token)\n",
    "\n",
    "    return tokens\n",
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create two sets of pre-processed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this configuration:\n",
    "# Don't preprocess the text, except to tokenize \n",
    "config_1 = {\n",
    "  \"stem\": False,\n",
    "  \"remove_stopwords\" : False,\n",
    "  \"lowercase_text\": True\n",
    "} \n",
    "\n",
    "\n",
    "# In this configuration:\n",
    "# Preprocess the text: stem and remove stopwords\n",
    "config_2 = {\n",
    "  \"stem\": True,\n",
    "  \"remove_stopwords\" : True,\n",
    "  \"lowercase_text\": True, \n",
    "} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now process the documents and queries according to the configuration specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "doc_repr_1 = []\n",
    "doc_repr_2 = []\n",
    "for (doc_id, document) in docs:\n",
    "    doc_repr_1.append((doc_id, process_text(document, **config_1)))\n",
    "    doc_repr_2.append((doc_id, process_text(document, **config_2)))  \n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--- \n",
    "\n",
    "## Section 2: Indexing (10 points)\n",
    "\n",
    "\n",
    "A retrieval function usually takes in a query document pair, and scores a query against a document.  Our document set is quite small - just a few thousand documents. However, consider a web-scale dataset with a few million documents. In such a scenario, it would become infeasible to score every query and document pair. In such a case, we can build an inverted index. From Wikipedia:\n",
    "\n",
    "> ... , an inverted index (also referred to as a postings file or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, .... The purpose of an inverted index is to allow fast full-text searches, at a cost of increased processing when a document is added to the database. ...\n",
    "\n",
    "\n",
    "Consider a simple inverted index, which maps from word to document. This can improve the performance of a retrieval system significantly. In this assignment, we consider a *simple* inverted index, which maps a word to a set of documents. In practice, however, more complex indices might be used.  \n",
    "\n",
    "\n",
    "### Building an index\n",
    "\n",
    "A retrieval function usually takes in a query document pair, and scores a query against a document.  Our document set is quite small - just a few thousand documents. However, consider a web-scale dataset with a few million documents. In such a scenario, it would become infeasible to score every query and document pair. In such a case, we can build an inverted index. From Wikipedia:\n",
    "\n",
    "> ... , an inverted index (also referred to as a postings file or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, .... The purpose of an inverted index is to allow fast full-text searches, at a cost of increased processing when a document is added to the database. ...\n",
    "\n",
    "\n",
    "Consider a simple inverted index, which maps from word to document. This can improve the performance of a retrieval system significantly. In this assignment, we consider a *simple* inverted index, which maps a word to a set of documents. In practice, however, more complex indices might be used.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In this assignment we will be using an index created in memory, since our dataset is tiny. To get started, build a simple index that maps simply counts the number of tokens present in a document. This index  is built using a python dictionary.\n",
    "\n",
    "*Answer the following questions*:\n",
    "- What is the time complexity of retrieving a list of documents from a python `dict` object? \n",
    "    - Note that this answer assumes a query length of 1.\n",
    "    Assuming the dictionary to be an inverted index then the keys are the tokens and we only have to access the dictionary once to retrieve the document list. Accessing an element in a python dict is in constant time, i.e. $O(1)$, because a dict is implemented as a hash table. \n",
    "    Assuming the dictionary is maps from document id to tokens in the document, we have to access the dictionary multiple times. We would have a total complexity of $O(D\\cdot n)$ where $D$ is the number of documents and $n$ is the average length of a word list associated with a document. This is because searching a list of length $n$ is in $O(n)$, assuming the list is not ordered. We have to search the token list for each document.  Note that the average runtime of this could be reduced in practice if the list of words associated with a document was in alphabetical order.\n",
    "    \n",
    "- Consider the case with a 10 million documents. What is the time complexity of retrieval with an inverted index (assuming you can fit the entire index in memory)? (Hint: Consider length of a query $|q|$) \n",
    "\n",
    "    - If we have an inverted index, the time complexity of retrieving a document list for a query is independent of the number of documents, but instead it dependents on the query length. We would need to access the index dictionary $|q|$ times, i.e. we have $O(|q|)$ (note that we do not account here for having to calculate the intersection between all retrieved document id lists). This reduces the time complexity compared to the {document-> token} dictonary a lot, since the query length is usually much smaller than the number of documents.\n",
    "    \n",
    "- For a large enough collection, we cannot store an index in memory. How is this tackled in practice (briefly explain)? Comment on the time complexity. \n",
    "    - If the inverted index is too big for our runtime memory, we can make use of merging. Instead of having one index, we write multiple partial indices to the disk. At retrieval time we load one index after the other and store the partial result, and then we merge all partial results together. Regarding time complexity, this does only add a merging operation in the end, which can happen in $O($*total number of retrived document ids*$)$ using the python *set* function.\n",
    "    \n",
    "- Mention a use-case in which a simple index (from word -> doc_id) might not suffice anymore. How would you modify the index to suit this use-case (if you can!)  \n",
    "    - An example information that could be stored in the posting next to the document ID is the word count. It is helpful in finding relevant documents because it allows us to distinguish documents that focus on a specific topic from those who just briefly mention it. As an example, we can think of the query \"Spaghetti Carbonara\". A document that describes how to make the dish is very likely to repeat both words multiple times whereas some document about planning your vacation in Italy might address in one sentence a restaurant where a nice Spaghetti Carbonara can be found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! 10 points\n",
    "def build_tf_index(documents):\n",
    "    \"\"\"\n",
    "    Build an inverted index (with counts). The output is a dictionary which takes in a token\n",
    "    and returns a list of (doc_id, count) where 'count' is the count of the 'token' in 'doc_id'\n",
    "    Input: a list of documents - (doc_id, tokens) \n",
    "    Output: An inverted index. [token] -> [(doc_id, token_count)]\n",
    "    \"\"\"\n",
    "    inverted_index = {}\n",
    " \n",
    "    for doc_id, tokens in documents:\n",
    "        token_counts = {}\n",
    "        for token in tokens:\n",
    "            if token not in token_counts.keys():\n",
    "                token_counts[token] = 0 \n",
    "            token_counts[token] += 1  \n",
    "\n",
    "        for token, count in token_counts.items():\n",
    "            if token not in inverted_index.keys(): \n",
    "                inverted_index[token] = [] \n",
    "            inverted_index[token].append((doc_id, count))\n",
    "\n",
    "    return inverted_index\n",
    " \n",
    "# Create the 2 indices\n",
    "tf_index_1 = build_tf_index(doc_repr_1)\n",
    "tf_index_2 = build_tf_index(doc_repr_2)\n",
    "\n",
    "# This function returns the correct index \n",
    "def get_index(index_set):\n",
    "    assert index_set in {1, 2}\n",
    "    return {\n",
    "        1: tf_index_1,\n",
    "        2: tf_index_2\n",
    "    }[index_set]\n",
    "\n",
    "# This function correctly pre-processes the text given the index set\n",
    "def preprocess_query(text, index_set):\n",
    "    assert index_set in {1, 2}\n",
    "    if index_set == 1:\n",
    "        return process_text(text, **config_1)\n",
    "    elif index_set == 2:\n",
    "        return process_text(text, **config_2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Section 3: Retrieval  (80 points)\n",
    "\n",
    "Now that we have cleaned and processed our dataset, we can start building simple IR systems. \n",
    "\n",
    "For now, we consider *simple* IR systems, which involve computing scores from the tokens present in the document/query. More advanced methods are covered in later assignments.\n",
    "\n",
    "We will implement the following methods in this section:\n",
    "- TF-IDF\n",
    "- BM25\n",
    "- Query Likelihood Models\n",
    "\n",
    "--- \n",
    "\n",
    "### Ranking functions\n",
    "\n",
    "\n",
    "Probably the simplest IR model is the Bag of Words (BOW) model. Implement a function that scores a query against a document using this model.   \n",
    "\n",
    "Note that you can use either the count of the token or 'binarize' it i.e set the value equal to 1 if the token appears.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Make sure you use the `get_index` function to retrieve the correct index, and call `preprocess_query` with the correct index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! 10 points\n",
    "def bow_search(query, index_set):\n",
    "    \"\"\"\n",
    "        Perform a search over all documents with the given query. \n",
    "        Note #1: You have to use the `get_index` function created in the previous cells\n",
    "        Note #2: You can binarize the counts if you wish to\n",
    "        Input: \n",
    "            query - a (unprocessed) query\n",
    "            index_set - the index to use\n",
    "        Output: a list of (document_id, score), sorted in descending relevance to the given query \n",
    "    \"\"\"\n",
    "\n",
    "    index = get_index(index_set)\n",
    "    query_preprocessed = preprocess_query(query, index_set)\n",
    "\n",
    "    bow = {} \n",
    "    for token in query_preprocessed: \n",
    "        if token not in index:\n",
    "            continue\n",
    "        for doc_id, _ in index[token]: \n",
    "            if doc_id not in bow:\n",
    "                bow[doc_id] = {}\n",
    "            if token not in bow[doc_id]: \n",
    "                # binarize\n",
    "                bow[doc_id][token] = 1 \n",
    "                \n",
    "    relevance = [] \n",
    "    for doc_id, words in bow.items():\n",
    "        # score = how many words of set(query) are present in document\n",
    "        # i.e. binarized term frequency\n",
    "        score = sum(words.values())\n",
    "        relevance.append((doc_id, score))\n",
    "    relevance = sorted(relevance,key=lambda x:(-x[1],x[0]))  \n",
    "    relevance = [(t[0],str(t[1])) for t in relevance]\n",
    "\n",
    "    return relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer the following questions*: \n",
    "- The BOW model is might not be a good choice for use in IR. Why? \n",
    "    - The number of occurences of a word within a document (i.e. the *word count / term-frequency*) is on its own not a reliable measure of how representative the word is for the document's content. This is because even after removing very common stop words like 'a'/'the'/etc. , there are still words that occur more often within written language than others. One way to deal with this problem would be to binarize the term-frequency (i.e. set 1 for occurence and 0 for non-occurence). However, this way we completely disregard how often a term occurs within a document, which is an indicator of how representative a word is for the document's content. Another way to obtain a measure for how representative a word is for a document's content is to normalize the term-frequency (tf) by the inverse document frequency (idf). This way, we value the occurence of a word that is generally rare higher than for example the 3-times occurence of a very frequent word.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1025', '1'), ('1051', '1'), ('1086', '1'), ('1236', '1'), ('1238', '1')]\n",
      "BOW Results:\n",
      "Rank 0(1): A Method of Syntax-Checking ALGOL 60\\nA syntax che...\n",
      "Rank 1(1): Multiword List Items\\nThe list concept as original...\n",
      "Rank 2(1): A Proposal for Input-Output Conventions in ALGOL\\n...\n",
      "Rank 3(1): The SMART Automatic Document Retrieval System-An I...\n",
      "Rank 4(1): A Technique for Integrated Reports from a Multi-ru...\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "docs_by_id = dict(docs)\n",
    "def print_results(docs, len_limit=50):    \n",
    "    for i, (doc_id, score) in enumerate(docs):\n",
    "        doc_content = docs_by_id[doc_id].strip().replace(\"\\n\", \"\\\\n\")[:len_limit] + \"...\"\n",
    "        print(f\"Rank {i}({score:.2}): {doc_content}\")\n",
    "\n",
    "test_bow = bow_search(\"report\", index_set=1)[:5]\n",
    "print(test_bow)\n",
    "print(f\"BOW Results:\")\n",
    "print_results(test_bow)\n",
    "#### replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we implement the tf-idf scoring functions, let's first write a function to compute the document frequencies of all words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! (5 points)\n",
    "def compute_df(documents):\n",
    "    \"\"\"\n",
    "        Compute the document frequency of all terms in the vocabulary\n",
    "        Input: A list of documents\n",
    "        Output: A dictionary with {token: document frequency)\n",
    "    \"\"\"\n",
    "    # document frequency = number of documents in the \n",
    "    # collection that contain certain token\n",
    "    df = {}\n",
    "    for document in documents: \n",
    "        for token in list(set(document)): \n",
    "            if token not in df.keys():\n",
    "                df[token] = 0\n",
    "            df[token] += 1 \n",
    "                        \n",
    "    return df\n",
    "    \n",
    "\n",
    "# get the document frequencies of each document\n",
    "df_1 = compute_df([d[1] for d in doc_repr_1])\n",
    "df_2 = compute_df([d[1] for d in doc_repr_2])\n",
    "\n",
    "def get_df(index_set):\n",
    "    assert index_set in {1, 2}\n",
    "    return {\n",
    "        1: df_1,\n",
    "        2: df_2\n",
    "    }[index_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement a function that computes a tf-idf score given a query.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! 10 points\n",
    "def tfidf_search(query, index_set):\n",
    "    \"\"\"\n",
    "        Perform a search over all documents with the given query using tf-idf. \n",
    "        Note #1: You have to use the `get_index` (and the `get_df`) function created in the previous cells\n",
    "        Input: \n",
    "            query - a (unprocessed) query\n",
    "            index_set - the index to use\n",
    "        Output: a list of (document_id, score), sorted in descending relevance to the given query \n",
    "    \"\"\"\n",
    "    \n",
    "    index = get_index(index_set) # {token: [('doc_id',count)]}\n",
    "    df = get_df(index_set) # {token: document_frequency}\n",
    "    query_preprocessed = preprocess_query(query, index_set) #[token]\n",
    "       \n",
    "    N = len(read_cacm_docs())\n",
    "    # note: using natural logarithm \n",
    "    idf = {token: np.log(N/doc_freq) for token, doc_freq in df.items()}\n",
    "    \n",
    "    \n",
    "    # term frequency = #occurrences of token t in document d\n",
    "    tf = {} # {doc_id : {token: count}}\n",
    "    \n",
    "    for token in query_preprocessed: \n",
    "        \n",
    "        # if token in none of the documents\n",
    "        if token not in index: continue\n",
    "        # else\n",
    "        for doc_id, count in index[token]:\n",
    "            if doc_id not in tf: \n",
    "                tf[doc_id] = {}\n",
    "            if token not in tf[doc_id]: \n",
    "                tf[doc_id][token] = count \n",
    "          \n",
    "    relevance = [] \n",
    "    for doc_id, words in tf.items(): # for every document\n",
    "        score = sum([tf * idf[word] for word, tf in words.items()])\n",
    "        relevance.append((doc_id, score))\n",
    "    relevance = sorted(relevance,key=lambda x:(-x[1],x[0]))\n",
    "    relevance = [(t[0],str(t[1])) for t in relevance]\n",
    "\n",
    "    return relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Results:\n",
      "Rank 0(23): Rejuvenating Experimental Computer Science\\nThis r...\n",
      "Rank 1(19): An Information Algebra - Phase I Report-Language\\n...\n",
      "Rank 2(11): ALGOL 60 Confidential\\nThe ALGOL 60 Report,* when ...\n",
      "Rank 3(7.): A Fortran Technique for Simplifying Input to Repor...\n",
      "Rank 4(7.): Control Procedures for Data Communication-An ASA P...\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "test_tfidf = tfidf_search(\"report\", index_set=1)[:5]\n",
    "print(f\"TFIDF Results:\")\n",
    "print_results(test_tfidf)\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer the following questions*: \n",
    "- It is generally not advisable to use the naive version of tf-idf. Why?\n",
    "    - Using the naive version of tf-idf the importance of the document scales linearly with the number of term occurences. This means that we will generally consider longer documents to be more relevant because they tend to repeat the same words multiple times.\n",
    "    \n",
    "    \n",
    "- Illustrate with an example why using a sublinear scaling for TF is preferable in some cases.  \n",
    "    - Consider the following extreme example: suppose we were to take a document d and create a new document d' by simply appending a copy of d to itself. While d' should be no more relevant to any query than d is, the use of the naive version of tf-idf would assign it twice as high a score as d. It might be preferable to sublinear scaling, i.e. using the logarithm of the term frequency instead of the raw term frequency, to make sure that the document's importance does not scale linearily with the term frequency. In the given example, this would mean that if the relevance of document d would be 1, the relevance of d' would be log(2) instead of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "*Answer the following questions*: \n",
    "- An alternative way to compute a query<>document score is to vectorize both the query and document (where each dimension corresponds to a token), and compute a score. The score can be computed using a dot product between the query and the document vectors. Why is the cosine similary function a better choice, particularly in IR? \n",
    "    - The cosine similarity between two vector representations measures similarity as the cosine of the angle between the two length-normalized vector representations. This measure is favorable compared to the naive tf-idf scores, because it disregards the magnitude of the vector difference. In contrast, it measures the difference between the relative distributions of terms in the document. If we treat a query as a short document, we can also represent it as vector and then rank the documents by cosine similarity of their vector representation to the query representation. \n",
    "    \n",
    "    \n",
    "- What is the time complexity of a search if we are using the vector space method mentioned in the previous question? What is the time complexity if we're using an index (assume that it fits in memory)? Assume $N$ is the number of documents and $|q|$ is the length of a query. \n",
    "     - For both searches we first have to obtain the weights for each term and document. This could be the tf-idf weights but also other measures are possible. In case of the index-based ranking, we first have to summarize the document-term scores in one score per document, which is linear in the number of documents, and then sort the scores, which has complexity $O(N \\cdot \\log{N})$ when using f.e. Timsort. This gives us a total complexity of $O(N+N \\cdot \\log{N})$. For the cosine similarity search, we have to calculate the dot product between every document vector in the collection and the vector representation of the query. This can get very expensive, considering that one cosine similarity calculation entails a dot product in the dimension of the number of different tokens in the collection (which can easily be in the ten-thousands). Furthermore, we also have to perform the sorting operation afterwards. This gives a full complexity of $O(N \\cdot T +N \\cdot \\log{N})$, where $T$ is the number of distinct tokens in the whole collection.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "#### Query Likelihood Models\n",
    "\n",
    "In this section you will implement a simple query likelihood model. \n",
    "\n",
    "First, let use implement a naive version of a QL model, assuming a multinomial unigram language model (with a uniform prior over the documents). \n",
    "\n",
    "**Note:** Make sure you use the `get_index` function to retrieve the correct index, and call `preprocess_query` with the correct index!\n",
    "\n",
    "--- \n",
    "\n",
    "*Answer the following questions*: \n",
    "- Write down the formula for computing the query likelihood, assuming a multinomial unigram language model. \n",
    "    - \\begin{align}P(Q|D)=\\prod_{i=1}^nP(q_i|D)\\end{align} where $q_i$ is a query word, and there are $n$ words in the query. The obvious choice for the language model probabilities $P(q_i|D)$ would be the maximumlikelihood estimate, i.e. \\begin{align}P(q_i|D)=\\frac{f_{q_i,D}}{|D|}\\end{align} with $f_{q_i,D}$ the count of $q_i$ in $D$ and $|D|$ the number of words in $D$.\n",
    "- What problem does this naive method have? Suggest a simple way to fix it.\n",
    "    - The problem is that as soon as any word of the query $Q$ is missing in a document $D$, the score from the query likelihood model, i.e. $P(Q|D)$, is zero. This is problemtic because especially for longer queries it is likely that the document will not contain all the query words which results in that a lot of documents will receive a relevance score of zero. Also we cannot distinguish between documents that miss different numbers of query words: e.g. the relevance score for documents missing 6 out of 7 query words will be zero, while documents missing only 1 query word will also get a score of zero. A simple way to fix this is to do smoothing: We lower the probablity estimates for words seen in the document text and then distribute the leftover probability to the words that are not present:\n",
    "    \\begin{align*}\n",
    "        (1-\\alpha_{D})P(q_i | D) + \\alpha_D P(q_i|C)\n",
    "    \\end{align*}\n",
    "    Here $\\alpha_D$ is a coefficient that controls the probability assigned to unseen words.\n",
    "    \\begin{align}\\end{align} We make use of the probability of query words in the document collection $C$, i.e.\\begin{align}P(q_i|C) = \\frac{f_{q_i,C}}{|C|} \\end{align}with $f_{q_i,C}$ the count of $q_i$ in the collection of all documents and $|C|$ the number of words in all documents. \n",
    "    We can now decide to set $\\alpha_D$ to a constant $0 \\leq \\lambda \\leq 1$.\n",
    "    This results in the *Jelinek-Mercer* method where the score for a query term is given by:\n",
    "    \\begin{align}P\\left(q_{i} | D\\right)=(1-\\lambda) \\frac{f_{q_{i}, D}}{|D|}+\\lambda \\frac{f_{q_i,C}}{|C|} \\end{align} with $\\lambda$ determining the degree of smoothing. We used this form of smoothing in our implementation\n",
    "    \n",
    "    In another approach known as *Dirichlet* smoothing, $\\alpha_D$ is not just simply a constant, but is dependent on document length:\n",
    "       \\begin{align*}\n",
    "           \\alpha_D = \\frac{\\mu}{|D|+\\mu}\n",
    "       \\end{align*}\n",
    "       with $\\mu$ as hyperparameter that needs to be tuned empirically.\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "def doc_lengths(documents):\n",
    "    doc_lengths = {doc_id:len(doc) for (doc_id, doc) in documents}\n",
    "    return doc_lengths\n",
    "\n",
    "doc_lengths_1 = doc_lengths(doc_repr_1)\n",
    "doc_lengths_2 = doc_lengths(doc_repr_2)\n",
    "\n",
    "def get_doc_lengths(index_set):\n",
    "    assert index_set in {1, 2}\n",
    "    return {\n",
    "        1: doc_lengths_1,\n",
    "        2: doc_lengths_2\n",
    "    }[index_set]\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! 15 points\n",
    "def naive_ql_search(query, index_set):\n",
    "    \"\"\"\n",
    "        Perform a search over all documents with the given query using a naive QL model. \n",
    "        Note #1: You have to use the `get_index` (and get_doc_lengths) function created in the previous cells\n",
    "        Input: \n",
    "            query - a (unprocessed) query\n",
    "            index_set - the index to use\n",
    "        Output: a list of (document_id, score), sorted in descending relevance to the given query \n",
    "    \"\"\"\n",
    "    index = get_index(index_set) # {token: [('doc_id',count)]}\n",
    "    doc_lengths = get_doc_lengths(index_set) # {doc_id: len(doc)}\n",
    "    query_preprocessed = preprocess_query(query, index_set) #[token]\n",
    "    \n",
    "    # scores for every document and query_word\n",
    "    scores = {i : {idx: 0 for idx, _ in enumerate(query_preprocessed)} \n",
    "                      for i in doc_lengths.keys()}\n",
    "    \n",
    "    \n",
    "    for idx, q_i in enumerate(query_preprocessed):\n",
    "        \n",
    "        # if query word not in any document\n",
    "        if q_i not in index: continue\n",
    "        \n",
    "        # else\n",
    "        for doc_id, count in index[q_i]:\n",
    "            scores[doc_id][idx] = count / doc_lengths[doc_id]\n",
    "    \n",
    "    relevance = [] \n",
    "    for doc_id, count_dict in scores.items():\n",
    "        score = np.prod(list(count_dict.values()))\n",
    "        relevance.append((doc_id, score))\n",
    "    \n",
    "    relevance = sorted(relevance,key=lambda x:(-x[1],x[0]))   \n",
    "    relevance = [(t[0],str(t[1])) for t in relevance]\n",
    "    return relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Results:\n",
      "Rank 0(0.): A CRT Report Generating System...\n",
      "Rank 1(0.): A Report Writer For COBOL...\n",
      "Rank 2(0.): ALGOL Sub-Committee Report - Extensions...\n",
      "Rank 3(0.): Supplement to the ALGOL 60 Report...\n",
      "Rank 4(0.): Report on the Algorithmic Language ALGOL 60...\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "test_naiveql = naive_ql_search(\"report\", index_set=1)[:5]\n",
    "print(f\"TFIDF Results:\")\n",
    "print_results(test_naiveql)\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement a (slightly more) complex QL model. This model should 'fix' the issue with the previous method. If your model requires hyperparameters, set a reasonable value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! 20 points\n",
    "def ql_search(query, index_set, l=0.1):\n",
    "    \"\"\"\n",
    "        Perform a search over all documents with the given query using a appropriate QL model. \n",
    "        Note #1: You have to use the `get_index` (and get_doc_lengths) function created in the previous cells\n",
    "        Note #2: You might have to create some variables beforehand and use them in this function\n",
    "        Input: \n",
    "            query - a (unprocessed) query\n",
    "            index_set - the index to use\n",
    "        Output: a list of (document_id, score), sorted in descending relevance to the given query \n",
    "    \"\"\"\n",
    "    index = get_index(index_set) # {token: [('doc_id',count)]}\n",
    "    doc_lengths = get_doc_lengths(index_set) # {doc_id: len(doc)}\n",
    "    query_preprocessed = preprocess_query(query, index_set) #[token]\n",
    "    \n",
    "    len_C = sum(doc_lengths.values()) # len of all documents\n",
    "    \n",
    "    # scores for every document and query_word\n",
    "    scores = {i : {idx: 0 for idx, _ in enumerate(query_preprocessed)} \n",
    "                      for i in doc_lengths.keys()}\n",
    "    \n",
    "    for idx, q_i in enumerate(query_preprocessed): # for every query word q_i\n",
    "        \n",
    "        # if query word not in collection \n",
    "        if q_i not in index: continue\n",
    "        \n",
    "        # count how often q_i occurs overall\n",
    "        count_C = 0\n",
    "        if q_i in index: \n",
    "            count_C = sum([count for _,count in index[q_i]])\n",
    "            \n",
    "        # calculate P(q_i|C) (same for all documents)\n",
    "        P_qi_C = count_C / len_C\n",
    "        \n",
    "        # for every document\n",
    "        for doc_id in doc_lengths.keys(): \n",
    "            scores[doc_id][idx] = l * P_qi_C\n",
    "                        \n",
    "        # update if doc contains q_i    \n",
    "        for doc_id, count in index[q_i]: \n",
    "            # caculate P(q_i|D)\n",
    "            P_qi_D = count / doc_lengths[doc_id]\n",
    "            scores[doc_id][idx] += (1-l) * P_qi_D \n",
    "            \n",
    "    relevance = [] \n",
    "    for doc_id, count_dict in scores.items():\n",
    "        \n",
    "        score = np.prod(list(count_dict.values()))\n",
    "        relevance.append((doc_id, score))\n",
    "    \n",
    "    relevance = sorted(relevance,key=lambda x:(-x[1],x[0]))   \n",
    "    relevance = [(t[0],str(t[1])) for t in relevance]\n",
    "\n",
    "    return relevance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0(0.): A CRT Report Generating System...\n",
      "Rank 1(0.): A Report Writer For COBOL...\n",
      "Rank 2(0.): ALGOL Sub-Committee Report - Extensions...\n",
      "Rank 3(0.): Supplement to the ALGOL 60 Report...\n",
      "Rank 4(0.): Report on the Algorithmic Language ALGOL 60...\n",
      "\n",
      "Rank 0(3.): A CRT Report Generating System...\n",
      "Rank 1(3.): A Report Writer For COBOL...\n",
      "Rank 2(3.): ALGOL Sub-Committee Report - Extensions...\n",
      "Rank 3(5.): Supplement to the ALGOL 60 Report...\n",
      "Rank 4(1.): Report on the Algorithmic Language ALGOL 60...\n"
     ]
    }
   ],
   "source": [
    "#### Test the QL model\n",
    "test_ql_results = ql_search(\"report\", index_set=1)[:5]\n",
    "print_results(test_ql_results)\n",
    "print()\n",
    "test_ql_results_long = ql_search(\"report \" * 10, index_set=1)[:5]\n",
    "print_results(test_ql_results_long)\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer the following questions*: \n",
    "- What happens to the query likelihood for long queries? What is a simple fix for this issue?\n",
    "    - Recall that we calculate the query likelihood by:\n",
    "    \\begin{align}P(Q|D)=\\prod_{i=1}^nP(q_i|D)\\end{align} \n",
    "    \n",
    "    Important to note is that we multiply over probabilities, which are values in range $[0, 1]$. This can lead to numerical underflows if the query is long ($n$ is large). A simple fix is to calculate the query log likelihood instead. This score is rank-equivalent and allows us to get rid of the product: \n",
    "    \\begin{align}\\log{P(Q|D)}=\\sum_{i=1}^n \\log{P(q_i|D)}\\end{align} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--- \n",
    "\n",
    "#### BM25\n",
    "\n",
    "In this section, we will implement the widely used and hard to beat BM25 scoring function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! (20 points)\n",
    "def bm25_search(query, index_set, k1=0.5, b=1):\n",
    "    \"\"\"\n",
    "        Perform a search over all documents with the given query using BM25. \n",
    "        Note #1: You have to use the `get_index` (and `get_doc_lengths`) function created in the previous cells\n",
    "        Note #2: You might have to create some variables beforehand and use them in this function\n",
    "        Input: \n",
    "            query - a (unprocessed) query\n",
    "            index_set - the index to use\n",
    "        Output: a list of (document_id, score), sorted in descending relevance to the given query \n",
    "    \"\"\"\n",
    "    index = get_index(index_set) # {token: [('doc_id',count)]}\n",
    "    doc_lengths = get_doc_lengths(index_set) # {doc_id: len(doc)}\n",
    "    query_preprocessed = preprocess_query(query, index_set) #[token]\n",
    "    df = get_df(index_set) # {token: document_frequency}\n",
    "    \n",
    "    # NOTE: Implementation according to ITIR book (Manning et al.) formula 11.32\n",
    "    \n",
    "    # ifd weights for every token\n",
    "    N = len(doc_lengths)\n",
    "    idf = {token: np.log(N/doc_freq) for token, doc_freq in df.items()}\n",
    "    \n",
    "    # average document length\n",
    "    L_avg = sum(doc_lengths.values())/N\n",
    "    \n",
    "    # scores for every document and query_word\n",
    "    scores = {d : [] for d in doc_lengths.keys()}\n",
    "    \n",
    "    # for every token in the query\n",
    "    for token in query_preprocessed:\n",
    "                    \n",
    "        # if query token in none of the documents\n",
    "        if token not in index: continue\n",
    "        # else\n",
    "        for doc_id, count in index[token]: \n",
    "            L_d = doc_lengths[doc_id]\n",
    "            tf = count\n",
    "            scores[doc_id].append((idf[token]*(k1+1)*tf)/(k1*(1-b+b*(L_d/L_avg))+tf))\n",
    "            \n",
    "    relevance = [] \n",
    "    for doc_id, S in scores.items():\n",
    "        score = sum(S)\n",
    "        relevance.append((doc_id, score))\n",
    "        \n",
    "    relevance = sorted(relevance,key=lambda x:(-x[1],x[0]))   \n",
    "    relevance = [(t[0],str(t[1])) for t in relevance]\n",
    "\n",
    "    return relevance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0(5.): A CRT Report Generating System...\n",
      "Rank 1(5.): A Report Writer For COBOL...\n",
      "Rank 2(5.): ALGOL Sub-Committee Report - Extensions...\n",
      "Rank 3(5.): Supplement to the ALGOL 60 Report...\n",
      "Rank 4(5.): Report on the Algorithmic Language ALGOL 60...\n"
     ]
    }
   ],
   "source": [
    "#### Test the BM25 model\n",
    "test_bm25_results = bm25_search(\"report\", index_set=1)[:5]\n",
    "print_results(test_bm25_results)\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "*Answer the following questions*: \n",
    "- Briefly explain how the BM25 model improves upon the tf-idf model.\n",
    "    - The BM25 model is able to normalize the term frequency by the document length, preventing overestimation of the relevance of longer documents.\n",
    "    \n",
    "    Recall that the tf-idf model was defined:\n",
    "    \\begin{align}\n",
    "        \\mathrm{\\sum_{t \\in q} tf_{t,d} \\times idf_{t}} \n",
    "    \\end{align}\n",
    "    \n",
    "    The BM25 model is defined as follows (Manning *et al.*):\n",
    "   \n",
    "    \\begin{align}\n",
    "    \\sum_{t \\in q} \\log \\left[\\frac{N}{\\mathrm{df}_{t}}\\right] \\cdot \\frac{\\left(k_{1}+1\\right) \\mathrm{tf}_{t d}}{k_{1}\\left((1-b)+b \\times\\left(L_{d} / L_{\\mathrm{ave}}\\right)\\right)+\\mathrm{tf}_{t d}}\n",
    "    \\end{align}\n",
    "    \n",
    "    Cited from Manning *et al.*: Here, $\\mathrm{tf_{td}}$ is the frequency of term $t$ in document $d$, and $\\mathrm{L_d}$ and $\\mathrm{L_{ave}}$ are the length of document $d$ and the average document length for the whole collection. The variable $\\mathrm{k_1}$ is a positive tuning parameter that calibrates the document term frequency scaling. A $\\mathrm{k_1}$ value of 0 corresponds to a binary model (no term frequency), and a large value corresponds to using raw term frequency. $b$ is another tuning parameter $(0  b  1)$ which determines the scaling by document length: $b = 1$ corresponds to fully scaling the term weight by the document length, while $b = 0$ corresponds to no length normalization.\n",
    "\n",
    "    In the BM25 model, the first term is the inverse document frequency (idf) like in the tf-idf model. The difference between the models lies in the second term: As discussed earlier, longer documents tend to have higher term frequencies while not being necessarily more relevant. The tf-idf model just uses the raw term frequency (tf), whereas the BM25 model is able to normalize the tf component by the document length. The normalizing part in the tf component is $k_{1}\\left((1-b)+b \\times\\left(L_{d} / L_{\\mathrm{ave}}\\right)\\right)$ where $b$ regulates the scaling of the document lenght. So for an appropriate value of $b$, this normalization helpes to make the relevance score independent of the document length. Also for certain $k_1$, the effect of $\\mathrm{tf_{td}}$ can be made very non-linear. This gives a similar effect as the *sublinear scaling for TF* discussed previously, i.e. after the term occured a few times in the document additional occurences will have little impact on the relevance score. \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Highlighter function\n",
    "# class for results\n",
    "ResultRow = namedtuple(\"ResultRow\", [\"doc_id\", \"snippet\", \"score\"])\n",
    "# doc_id -> doc\n",
    "docs_by_id = dict((d[0], d[1]) for d in docs)\n",
    "\n",
    "def highlight_text(document, query, tol=17):\n",
    "    import re\n",
    "    tokens = tokenize(query)\n",
    "    regex = \"|\".join(f\"(\\\\b{t}\\\\b)\" for t in tokens)\n",
    "    regex = re.compile(regex, flags=re.IGNORECASE)\n",
    "    output = \"\"\n",
    "    i = 0\n",
    "    for m in regex.finditer(document):\n",
    "        start_idx = max(0, m.start() - tol)\n",
    "        end_idx = min(len(document), m.end() + tol)\n",
    "        output += \"\".join([\"...\",\n",
    "                        document[start_idx:m.start()],\n",
    "                        \"<strong>\",\n",
    "                        document[m.start():m.end()],\n",
    "                        \"</strong>\",\n",
    "                        document[m.end():end_idx],\n",
    "                        \"...\"])\n",
    "    return output.replace(\"\\n\", \" \")\n",
    "\n",
    "\n",
    "def make_results(query, search_fn, index_set):\n",
    "    results = []\n",
    "    for doc_id, score in search_fn(query, index_set):\n",
    "        highlight = highlight_text(docs_by_id[doc_id], query)\n",
    "        if len(highlight.strip()) == 0:\n",
    "            highlight = docs_by_id[doc_id]\n",
    "        results.append(ResultRow(doc_id, highlight, score))\n",
    "    return results\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "The widget below allows you to play with the search functions you've written so far. This can be used, for example, to answer some of the theory questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec79c8d38573496ab45ee4af251309da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Search Bar')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set this to the function you want to test\n",
    "# this function should take in a query (string)\n",
    "# and return a sorted list of (doc_id, score) \n",
    "# with the most relevant document in the first position\n",
    "search_fn = bm25_search\n",
    "\n",
    "index_set = 1\n",
    "\n",
    "text = widgets.Text(description=\"Search Bar\", width=200)\n",
    "display(text)\n",
    "\n",
    "def handle_submit(sender):\n",
    "    print(f\"Searching for: '{sender.value}'\")\n",
    "    \n",
    "    results = make_results(sender.value, search_fn, index_set)\n",
    "    \n",
    "    # display only the top 5\n",
    "    results = results[:5]\n",
    "    \n",
    "    body = \"\"\n",
    "    for idx, r in enumerate(results):\n",
    "        body += f\"<li>Document #{r.doc_id}({r.score}): {r.snippet}</li>\"\n",
    "    display(HTML(f\"<ul>{body}</ul>\"))\n",
    "    \n",
    "\n",
    "text.on_submit(handle_submit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Offline Evaluation (45 points)\n",
    "\n",
    "Before we jump in and implement an algorithm for retrieval, we first have to learn how to evaluate such a system. In particular, we will work with offline evaluation metrics. These metrics are computed on a dataset with known relevance judgements.\n",
    "\n",
    "Implement the following evaluation metrics. \n",
    "\n",
    "1. Precision\n",
    "2. Recall\n",
    "3. Mean Average Precision\n",
    "4. Expected Reciprocal Rank\n",
    "\n",
    "---\n",
    "*Answer the following questions*: \n",
    "- What are the main limitations of an offline evaluation?\n",
    "    - In order to perform offline evaluation we need a test collection where documents are labeled e.g. in the binary case as relevant or nonrelevant. These labels are the gold standard or ground truth judgement assigned by human annotators (judges). However, sometimes it is hard to assess the relevance of a document relative to an information need. Take for example the query \"Paris\". Based on the query alone it is unclear whether the user is looking for the city Paris or Paris Hilton. So judges would annotate documents on the topic of city Paris and Paris Hilton both as relevant since the information need is ambiguous. In an online setting we could try to infer the information need based on additional information from for example the current search session. Based on previous searches or clicks of the user, we might find out that the user means Paris Hilton which allows us to score documents related to the city Paris as less relevant.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's take a look at the `qrels.text` file, which contains the ground truth relevance scores. The relevance labels for CACM are binary - either 0 or 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 1410  0 0\r\n",
      "01 1572  0 0\r\n",
      "01 1605  0 0\r\n",
      "01 2020  0 0\r\n",
      "01 2358  0 0\r\n",
      "02 2434  0 0\r\n",
      "02 2863  0 0\r\n",
      "02 3078  0 0\r\n",
      "03 1134  0 0\r\n",
      "03 1613  0 0\r\n"
     ]
    }
   ],
   "source": [
    "!head ./datasets/qrels.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is the `query_id` and the second column is the `document_id`. You can safely ignore the 3rd and 4th columns. Write a function to read in the file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this!\n",
    "def read_qrels(root_folder = \"./datasets/\"):\n",
    "    \"\"\"\n",
    "        Reads the qrels.text file. \n",
    "        Output: A dictionary: query_id -> [list of relevant documents]\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    with open(os.path.join(root_folder,\"qrels.text\")) as file:\n",
    "        lines = file.read().split('\\n')[:-1]\n",
    "        for l in lines:\n",
    "            split = l.split(' ')\n",
    "            key = str(int(split[0]))\n",
    "            if not key in res:\n",
    "                res[key] = []\n",
    "            res[key].append(split[1])\n",
    "    return res\n",
    "                \n",
    "qrels = read_qrels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "assert len(qrels) == 52, \"There should be 52 queries with relevance judgements\"\n",
    "assert sum(len(j) for j in qrels.values()) == 796, \"There should be a total of 796 Relevance Judgements\"\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implement the metrics below. \n",
    "\n",
    "**Note:** For a given query `query_id`, you can assume that documents *not* in `qrels[query_id]` are not relevant to `query_id`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! (10 points)\n",
    "def recall_k(results, relevant_docs, k):\n",
    "    \"\"\"\n",
    "        Compute Recall@K\n",
    "        Input: \n",
    "            results: A sorted list of 2-tuples (document_id, score), with the most relevant document in the first position\n",
    "            relevant_docs: A set of relevant documents. \n",
    "            k: the cut-off\n",
    "        Output: Recall@K\n",
    "    \"\"\"\n",
    "    return len([r for r in results[:k] if r[0] in relevant_docs])/len(relevant_docs)\n",
    "    \n",
    "    \n",
    "# TODO: Implement this! (10 points)\n",
    "def precision_k(results, relevant_docs, k):\n",
    "    \"\"\"\n",
    "        Compute Precision@K\n",
    "        Input: \n",
    "            results: A sorted list of 2-tuples (document_id, score), \n",
    "                    with the most relevant document in the first position\n",
    "            relevant_docs: A set of relevant documents. \n",
    "            k: the cut-off\n",
    "        Output: Precision@K\n",
    "    \"\"\"\n",
    "    return len([r for r in results[:k] if r[0] in relevant_docs]) / k\n",
    "    \n",
    "    \n",
    "# TODO: Implement this! (10 points)\n",
    "def average_precision(results, relevant_docs):\n",
    "    \"\"\"\n",
    "        Compute Average Precision (for a single query - the results are \n",
    "        averaged across queries to get MAP in the next few cells)\n",
    "        Hint: You can use the recall_k and precision_k functions here!\n",
    "        Input: \n",
    "            results: A sorted list of 2-tuples (document_id, score), with the most \n",
    "                    relevant document in the first position\n",
    "            relevant_docs: A set of relevant documents. \n",
    "        Output: Average Precision\n",
    "    \"\"\"\n",
    "    # averaging the precision values from the rank positions where a relevant document was retrieved \n",
    "    pos = [idx  for idx, (d, _) in enumerate(results) if d in relevant_docs]\n",
    "    \n",
    "    # if no relevant documents\n",
    "    if not pos: return 0\n",
    "    \n",
    "    precision_pos = [precision_k(results, relevant_docs, p+1) for p in pos]\n",
    "    \n",
    "    return np.average(precision_pos)\n",
    "\n",
    "\n",
    "# TODO: Implement this! (15 points)\n",
    "def err(results, relevant_docs):\n",
    "    \"\"\"\n",
    "        Compute the expected reciprocal rank.\n",
    "        Hint: https://dl.acm.org/doi/pdf/10.1145/1645953.1646033?download=true\n",
    "        Input: \n",
    "            results: A sorted list of 2-tuples (document_id, score), with the most \n",
    "                    relevant document in the first position\n",
    "            relevant_docs: A set of relevant documents. \n",
    "        Output: ERR\n",
    "    \"\"\"\n",
    "    # Note: This implementation is faster.\n",
    "    # We dont know if we can assume that R_i is either 1 or 0 (see linked document, chapter 5)\n",
    "    # r = len(results)\n",
    "    # for idx, (doc, _) in enumerate(results):\n",
    "    #     if doc in relevant_docs:\n",
    "    #        r = idx + 1\n",
    "    #        break \n",
    "    # res = 1 / r   \n",
    "    \n",
    "    # If we have  go with the original fomula with g in {0,1}:\n",
    "    res = 0\n",
    "    one_min_R_i = []\n",
    "    for r, (doc, _) in enumerate(results):\n",
    "        r_r = 0.5 if doc in relevant_docs else 0\n",
    "        res += (1/(r+1))*np.prod(one_min_R_i)*r_r\n",
    "        one_min_R_i.append(1-r_r)\n",
    "        \n",
    "    return res  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer the following questions*: \n",
    "- What are the main drawbacks of precision & recall?\n",
    "    - * Precision and recall are traditional evaluation measures where we treat the relevance as a binary classification problem. If we want to use recall and precision to evaluate the output of our retrieval model, we could calcuate precision and recall scores at every ranking. However, it is inconvenient and redundant to this for the large set of retrieved documents assuming that the user will only examine the top-K results. Therefore we could calculate recall and precision only for the top-K documents, i.e. we choose K as a cut-off point. Whereas Precision@K and Recall@K are easy to compute and to interpret, there are three main drawbacks:\n",
    "        - The value of K has a huge impact on the metric\n",
    "        - The ranking of documents withing the top K is inconsequential \n",
    "        - How do we pick a value for K?\n",
    "        \n",
    "        \n",
    "         \n",
    "- What problems with Precision@K does Average Precision solve? \n",
    "    -    With Average Precision we don't have to pick a cut-off K. \n",
    "         Average precision is calculated using only the ranking of retrieved relevant documents, resulting in a single value that strongly depends on the highly ranked relevant documents; the lower a relevant document is ranked, the less it will affect the average precision value. In this way, the measure is able to take into account that as many relevant documents as possible should be retrieved and that the most relevant documents should be ranked on top [Croft et al.].\n",
    "\n",
    "- The CACM dataset has *binary* relevance judgements. However, a more suitable way of assigning judgements is to use graded relevance. Mention a metric which might be more suitable for a graded relevance, and breifly explain why. \n",
    "    - A metric that is more suitable for graded relevance is discounted cumulative gain (DCG). This metric is based on the following two assumptions [Croft et al.]: Highly relevant documents are more useful than marginally relevant document (i.e. relevance of a document will not degrade if there is already another relevant document retrieved with exactly the same information). The lower a relevant document is ranked in the list, the less useful it is for the user, because it is less likely to be examined. Graded relevance is used as a measure of the usefulness, or gain from examining a document, rather than treating it as a binary classification task.\n",
    "    \n",
    "- Consider a text processing step: stemming. What effect does this have on metrics? (Hint: Try changing the pre-processing config and try it out!)\n",
    "    - If we do stemming in our text processing, term frequency counts in documents will go up. This causes the relevance score to be less accumulated on the top ranked documents but slightly more distributed.\n",
    "\n",
    "    - DCG is the weighted sum over the relevance of the ranked items where weight decreases when the rank increases. Therefore DCG scores for the ranking will be lower than without stemming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's define some metrics@k using [partial functions](https://docs.python.org/3/library/functools.html#functools.partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "recall_at_1 = partial(recall_k, k=1)\n",
    "recall_at_5 = partial(recall_k, k=5)\n",
    "recall_at_10 = partial(recall_k, k=10)\n",
    "precision_at_1 = partial(precision_k, k=1)\n",
    "precision_at_5 = partial(precision_k, k=5)\n",
    "precision_at_10 = partial(precision_k, k=10)\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following function evaluates a `search_fn` using the `metric_fn`. Note that the final number is averaged over all the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "def evaluate_search_fn(search_fn, metric_fn, index_set):\n",
    "    # build a dict query_id -> query \n",
    "    queries_by_id = dict((q[0], q[1]) for q in queries)\n",
    "    \n",
    "    metrics = np.zeros(len(qrels), dtype=np.float32)\n",
    "    for i, (query_id, relevant_docs) in enumerate(qrels.items()):\n",
    "        query = queries_by_id[query_id]\n",
    "        results = search_fn(query, index_set)\n",
    "        # Results: list((doc-id,relevance)), relevant_docs: list(relevant docs)\n",
    "        metrics[i] = metric_fn(results, relevant_docs)\n",
    "    \n",
    "    return metrics.mean()\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 1\n",
      "\tEvaluating Search Function: NaiveQL\n",
      "\t\tMetric: ERR: 0.023367108777165413\n",
      "\t\tMetric: MAP: 0.011317642405629158\n",
      "\t\tMetric: Recall@1: 0.002950174966827035\n",
      "\t\tMetric: Recall@5: 0.0064466786570847034\n",
      "\t\tMetric: Recall@10: 0.0064466786570847034\n",
      "\t\tMetric: Precision@1: 0.03846153989434242\n",
      "\t\tMetric: Precision@5: 0.015384615398943424\n",
      "\t\tMetric: Precision@10: 0.007692307699471712\n",
      "\n",
      "\tEvaluating Search Function: QL\n",
      "\t\tMetric: ERR: 0.18586169183254242\n",
      "\t\tMetric: MAP: 0.09984731674194336\n",
      "\t\tMetric: Recall@1: 0.025460688397288322\n",
      "\t\tMetric: Recall@5: 0.07924629747867584\n",
      "\t\tMetric: Recall@10: 0.09881102293729782\n",
      "\t\tMetric: Precision@1: 0.23076923191547394\n",
      "\t\tMetric: Precision@5: 0.14615385234355927\n",
      "\t\tMetric: Precision@10: 0.11346153914928436\n",
      "\n",
      "\tEvaluating Search Function: BM25\n",
      "\t\tMetric: ERR: 0.39577144384384155\n",
      "\t\tMetric: MAP: 0.23581303656101227\n",
      "\t\tMetric: Recall@1: 0.08775953203439713\n",
      "\t\tMetric: Recall@5: 0.2031344324350357\n",
      "\t\tMetric: Recall@10: 0.2668766677379608\n",
      "\t\tMetric: Precision@1: 0.5384615659713745\n",
      "\t\tMetric: Precision@5: 0.3153846561908722\n",
      "\t\tMetric: Precision@10: 0.2192307859659195\n",
      "\n",
      "\tEvaluating Search Function: BOW\n",
      "\t\tMetric: ERR: 0.1853068619966507\n",
      "\t\tMetric: MAP: 0.10312586277723312\n",
      "\t\tMetric: Recall@1: 0.03622356802225113\n",
      "\t\tMetric: Recall@5: 0.07273100316524506\n",
      "\t\tMetric: Recall@10: 0.10846737772226334\n",
      "\t\tMetric: Precision@1: 0.17307692766189575\n",
      "\t\tMetric: Precision@5: 0.13846153020858765\n",
      "\t\tMetric: Precision@10: 0.11153845489025116\n",
      "\n",
      "\tEvaluating Search Function: TF-IDF\n",
      "\t\tMetric: ERR: 0.1817854344844818\n",
      "\t\tMetric: MAP: 0.11046617478132248\n",
      "\t\tMetric: Recall@1: 0.03365948051214218\n",
      "\t\tMetric: Recall@5: 0.0866926908493042\n",
      "\t\tMetric: Recall@10: 0.13071545958518982\n",
      "\t\tMetric: Precision@1: 0.17307692766189575\n",
      "\t\tMetric: Precision@5: 0.13076922297477722\n",
      "\t\tMetric: Precision@10: 0.11346153169870377\n",
      "\n",
      "Index: 2\n",
      "\tEvaluating Search Function: NaiveQL\n",
      "\t\tMetric: ERR: 0.05614742264151573\n",
      "\t\tMetric: MAP: 0.02429136633872986\n",
      "\t\tMetric: Recall@1: 0.008185548707842827\n",
      "\t\tMetric: Recall@5: 0.014184451662003994\n",
      "\t\tMetric: Recall@10: 0.017680954188108444\n",
      "\t\tMetric: Precision@1: 0.09615384787321091\n",
      "\t\tMetric: Precision@5: 0.03846153989434242\n",
      "\t\tMetric: Precision@10: 0.023076923564076424\n",
      "\n",
      "\tEvaluating Search Function: QL\n",
      "\t\tMetric: ERR: 0.2590676546096802\n",
      "\t\tMetric: MAP: 0.14359799027442932\n",
      "\t\tMetric: Recall@1: 0.031006736680865288\n",
      "\t\tMetric: Recall@5: 0.09618329256772995\n",
      "\t\tMetric: Recall@10: 0.137273371219635\n",
      "\t\tMetric: Precision@1: 0.3461538553237915\n",
      "\t\tMetric: Precision@5: 0.23076923191547394\n",
      "\t\tMetric: Precision@10: 0.17884615063667297\n",
      "\n",
      "\tEvaluating Search Function: BM25\n",
      "\t\tMetric: ERR: 0.4531857669353485\n",
      "\t\tMetric: MAP: 0.315091073513031\n",
      "\t\tMetric: Recall@1: 0.12338612973690033\n",
      "\t\tMetric: Recall@5: 0.23654118180274963\n",
      "\t\tMetric: Recall@10: 0.31019699573516846\n",
      "\t\tMetric: Precision@1: 0.6346153616905212\n",
      "\t\tMetric: Precision@5: 0.38846153020858765\n",
      "\t\tMetric: Precision@10: 0.29615384340286255\n",
      "\n",
      "\tEvaluating Search Function: BOW\n",
      "\t\tMetric: ERR: 0.29131269454956055\n",
      "\t\tMetric: MAP: 0.15240822732448578\n",
      "\t\tMetric: Recall@1: 0.054822806268930435\n",
      "\t\tMetric: Recall@5: 0.11802177876234055\n",
      "\t\tMetric: Recall@10: 0.1490481197834015\n",
      "\t\tMetric: Precision@1: 0.36538460850715637\n",
      "\t\tMetric: Precision@5: 0.2230769246816635\n",
      "\t\tMetric: Precision@10: 0.15000000596046448\n",
      "\n",
      "\tEvaluating Search Function: TF-IDF\n",
      "\t\tMetric: ERR: 0.2889251112937927\n",
      "\t\tMetric: MAP: 0.18853217363357544\n",
      "\t\tMetric: Recall@1: 0.06173396483063698\n",
      "\t\tMetric: Recall@5: 0.12761031091213226\n",
      "\t\tMetric: Recall@10: 0.1907833218574524\n",
      "\t\tMetric: Precision@1: 0.32692307233810425\n",
      "\t\tMetric: Precision@5: 0.2153846174478531\n",
      "\t\tMetric: Precision@10: 0.17692309617996216\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index_sets = {1, 2}\n",
    "\n",
    "list_of_metrics = [\n",
    "    (\"ERR\", err),\n",
    "    (\"MAP\", average_precision),\n",
    "    (\"Recall@1\",recall_at_1),\n",
    "    (\"Recall@5\", recall_at_5),\n",
    "    (\"Recall@10\", recall_at_10),\n",
    "    (\"Precision@1\", precision_at_1),\n",
    "    (\"Precision@5\", precision_at_5),\n",
    "    (\"Precision@10\", precision_at_10)]\n",
    "\n",
    "list_of_search_fns = [\n",
    "    (\"NaiveQL\", naive_ql_search),\n",
    "    (\"QL\", ql_search),\n",
    "    (\"BM25\", bm25_search),\n",
    "    (\"BOW\", bow_search),\n",
    "    (\"TF-IDF\", tfidf_search)\n",
    "]\n",
    "\n",
    "\n",
    "results = {}\n",
    "for index_set in index_sets:\n",
    "    results[index_set] = {}\n",
    "    print(f\"Index: {index_set}\")\n",
    "    for search_fn_name, search_fn in list_of_search_fns:\n",
    "        print(f\"\\tEvaluating Search Function: {search_fn_name}\")\n",
    "        results[index_set][search_fn_name] = {}\n",
    "        for metric_name, metric_fn in list_of_metrics:\n",
    "            r = evaluate_search_fn(search_fn, metric_fn, index_set).mean()\n",
    "            print(f\"\\t\\tMetric: {metric_name}: {r}\")\n",
    "            results[index_set][search_fn_name][metric_name] = r\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Results and Analysis (20 points)\n",
    "\n",
    "The `results` dictionary contains the results for all search functions we implemented. Plot the results in bar charts, with clear labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'NaiveQL': {'ERR': 0.023367109,\n",
       "   'MAP': 0.011317642,\n",
       "   'Recall@1': 0.002950175,\n",
       "   'Recall@5': 0.0064466787,\n",
       "   'Recall@10': 0.0064466787,\n",
       "   'Precision@1': 0.03846154,\n",
       "   'Precision@5': 0.015384615,\n",
       "   'Precision@10': 0.0076923077},\n",
       "  'QL': {'ERR': 0.18586169,\n",
       "   'MAP': 0.09984732,\n",
       "   'Recall@1': 0.025460688,\n",
       "   'Recall@5': 0.0792463,\n",
       "   'Recall@10': 0.09881102,\n",
       "   'Precision@1': 0.23076923,\n",
       "   'Precision@5': 0.14615385,\n",
       "   'Precision@10': 0.11346154},\n",
       "  'BM25': {'ERR': 0.39577144,\n",
       "   'MAP': 0.23581304,\n",
       "   'Recall@1': 0.08775953,\n",
       "   'Recall@5': 0.20313443,\n",
       "   'Recall@10': 0.26687667,\n",
       "   'Precision@1': 0.53846157,\n",
       "   'Precision@5': 0.31538466,\n",
       "   'Precision@10': 0.21923079},\n",
       "  'BOW': {'ERR': 0.18530686,\n",
       "   'MAP': 0.10312586,\n",
       "   'Recall@1': 0.036223568,\n",
       "   'Recall@5': 0.072731,\n",
       "   'Recall@10': 0.10846738,\n",
       "   'Precision@1': 0.17307693,\n",
       "   'Precision@5': 0.13846153,\n",
       "   'Precision@10': 0.111538455},\n",
       "  'TF-IDF': {'ERR': 0.18178543,\n",
       "   'MAP': 0.110466175,\n",
       "   'Recall@1': 0.03365948,\n",
       "   'Recall@5': 0.08669269,\n",
       "   'Recall@10': 0.13071546,\n",
       "   'Precision@1': 0.17307693,\n",
       "   'Precision@5': 0.13076922,\n",
       "   'Precision@10': 0.11346153}},\n",
       " 2: {'NaiveQL': {'ERR': 0.056147423,\n",
       "   'MAP': 0.024291366,\n",
       "   'Recall@1': 0.008185549,\n",
       "   'Recall@5': 0.014184452,\n",
       "   'Recall@10': 0.017680954,\n",
       "   'Precision@1': 0.09615385,\n",
       "   'Precision@5': 0.03846154,\n",
       "   'Precision@10': 0.023076924},\n",
       "  'QL': {'ERR': 0.25906765,\n",
       "   'MAP': 0.14359799,\n",
       "   'Recall@1': 0.031006737,\n",
       "   'Recall@5': 0.09618329,\n",
       "   'Recall@10': 0.13727337,\n",
       "   'Precision@1': 0.34615386,\n",
       "   'Precision@5': 0.23076923,\n",
       "   'Precision@10': 0.17884615},\n",
       "  'BM25': {'ERR': 0.45318577,\n",
       "   'MAP': 0.31509107,\n",
       "   'Recall@1': 0.12338613,\n",
       "   'Recall@5': 0.23654118,\n",
       "   'Recall@10': 0.310197,\n",
       "   'Precision@1': 0.63461536,\n",
       "   'Precision@5': 0.38846153,\n",
       "   'Precision@10': 0.29615384},\n",
       "  'BOW': {'ERR': 0.2913127,\n",
       "   'MAP': 0.15240823,\n",
       "   'Recall@1': 0.054822806,\n",
       "   'Recall@5': 0.11802178,\n",
       "   'Recall@10': 0.14904812,\n",
       "   'Precision@1': 0.3653846,\n",
       "   'Precision@5': 0.22307692,\n",
       "   'Precision@10': 0.15},\n",
       "  'TF-IDF': {'ERR': 0.2889251,\n",
       "   'MAP': 0.18853217,\n",
       "   'Recall@1': 0.061733965,\n",
       "   'Recall@5': 0.12761031,\n",
       "   'Recall@10': 0.19078332,\n",
       "   'Precision@1': 0.32692307,\n",
       "   'Precision@5': 0.21538462,\n",
       "   'Precision@10': 0.1769231}}}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1080 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Implement this! (20 points)\n",
    "def subplot_results(results, metric, ax): \n",
    "    \n",
    "    # get results\n",
    "    result1 = [results[1][model][metric] for model in results[1]]\n",
    "    result2 = [results[2][model][metric] for model in results[2]]\n",
    "    \n",
    "    # barwidth \n",
    "    w = 0.3\n",
    " \n",
    "    r1 = np.arange(len(result1))\n",
    "    r2 = [x + w for x in r1]\n",
    "\n",
    "    # plot\n",
    "    ax.bar(r1, result1, color='#ff9900', width=w, edgecolor='white', label='index set 1')\n",
    "    ax.bar(r2, result2, color='#3366cc', width=w, edgecolor='white', label='index set 2')\n",
    "\n",
    "    # add labels\n",
    "    ax.set_xlabel(metric, fontweight='bold', fontsize=12)    \n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_xticks([r + w-0.15 for r in range(len(result1))])\n",
    "    ax.set_xticklabels(['NaiveQL', 'QL', 'BM25', 'BOW', 'TF-IDF'])\n",
    "    ax.legend()\n",
    "\n",
    "    \n",
    "def plot_results(results): \n",
    "    fig, axs = plt.subplots(4, 2, figsize=(10, 15))\n",
    "        \n",
    "    subplot_results(results, \"ERR\", ax=axs[0][0])\n",
    "    subplot_results(results, \"MAP\", ax=axs[0][1])\n",
    "    subplot_results(results, \"Recall@1\", ax=axs[1][0])\n",
    "    subplot_results(results, \"Recall@5\", ax=axs[1][1])\n",
    "    subplot_results(results, \"Recall@10\", ax=axs[2][0])\n",
    "    subplot_results(results, \"Precision@1\", ax=axs[2][1])\n",
    "    subplot_results(results, \"Precision@5\", ax=axs[3][0])\n",
    "    subplot_results(results, \"Precision@10\", ax=axs[3][1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a summary of what you observe in the results.\n",
    "You summary should compare results across the 2 indices and the methods being used. State what you expected to see in the results, followed by either supporting evidence *or* justify why the results did not support your expectations.      \n",
    "*Hint*: You may build upon the answers from the previous sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "The first thing to notice here is that using index set 2 is highly benefecial for the performance of all ranking algorithms. This makes sense, considering that in index set 1 we neither stemmed nor removed stop words, which means we cannot find relations between words of the same word stem and also give a big importance to high frequent fill words that are not elevant to the content.\n",
    "\n",
    "Regarding the ranking methods, Naive QL performed the worst. This makes sense when we consider that our test queries were quite long, therefore it is unlikely for a document to contain all query words and a lot of actually relevant documents get a score of 0. QL clearly outperformed Naive QL in all metrics, which is what we expected. However, compared to the simple methods tf-idf and BOW, QL had a similar precision and even a lower recall, ERR and MAP. This came as a surprise, because QL seemed to be a more sophisticated method than BOW and tf-idf by definition. We cannot really explain this to ourselves, maybe we should have experimented with different hyperparameters more.\n",
    "\n",
    "Finally, BM25 was our winning ranking algorithm across all metrics. This comes as no surprise when we consider that tf-idf already performs pretty well and BM25 extends tf-idf and makes it more independent of document-length and uneven term frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
